{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1 (Bernouilli)\n",
    "\n",
    "Let $y$ be a random variable that is drawn from a Bernouilli distribution with probability of success $p$. \n",
    "\n",
    "1. Consider a dataset $Y=(y_1,y_2,...,y_n)$ containing $n$ realizations of $y$. Write down the likelihood of $Y$ as a function of $p$.\n",
    "\n",
    "2. Assume that $p$ itself is drawn from the following distribution\n",
    "\n",
    "\\begin{equation*}\n",
    "p=\\left\\{ \n",
    "\\begin{array}{l}\n",
    "p_{h}\\text{ with probability } \\mu _{0}, \\\\ \n",
    "p_{l}\\text{ with probability } 1-\\mu _{0}.\n",
    "\\end{array}%\n",
    "\\right. \n",
    "\\end{equation*}\n",
    "\n",
    "The variable $\\mu_0$ denotes the prior in period $0$. Use Bayes rule to express the agent’s posterior $μ_1(0)$ at the beginning of period $2$ after having observed a failure in period $1$, i.e. $y_1=0$. Iterate the computation to derive the posterior after $n$ failures in a row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "1. The likelihood of $Y$ reads\n",
    "\n",
    "\\begin{equation*}\n",
    "    L\\left(p | Y \\right) = \\prod_{i = 1}^{n} p^{y_{i}}(1-p)^{1-y_{i}}.\n",
    "\\end{equation*}\n",
    "\n",
    "2. By Baye's rule, we can express $μ_1(0)$ as:\n",
    "\n",
    "\\begin{align*}\n",
    "    μ_1(0) = p(p_{h}|y_{1}=0) & = \\frac{p(y_{1}=0|p_{h})\\mu_{0}}{p(y_{1}=0|p_{h})\\mu_{0} + p(y_{1}=0|p_{l})(1-\\mu_{0})}\\\\\n",
    "                              & = \\frac{(1-p_{h})\\mu_{0}}{(1-p_{h})\\mu_{0} + (1-p_{l})(1-\\mu_{0})}.\n",
    "\\end{align*}\n",
    "\n",
    "The posterior after $n$ failures could be expressed as:\n",
    "\n",
    "\\begin{align*}\n",
    "    μ_n(0^n) = p(p_{h}|y_{1}=0, y_{2}=0,...,y_{n}=0) & = \\frac{p(y_{1}=0, y_{2}=0,...,y_{n}=0|p_{h})\\mu_{0}}{p(y_{1}=0, y_{2}=0,...,y_{n}=0|p_{h})\\mu_{0} + p(y_{1}=0, y_{2}=0,...,y_{n}=0|p_{l})(1-\\mu_{0})}\\\\\n",
    "                              & = \\frac{(1-p_{h})^{n}\\mu_{0}}{(1-p_{h})^{n}\\mu_{0} + (1-p_{l})^{n}(1-\\mu_{0})}.\n",
    "\\end{align*}\n",
    "\n",
    "Note that $μ_n$ can be expressed recursively\n",
    "\n",
    "\\begin{align*}\n",
    "    μ_n(0) = \\frac{(1-p_{h})\\mu_{n-1}}{(1-p_{h})^{n}\\mu_{n-1} + (1-p_{l})^{n}(1-\\mu_{n-1})}.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2 (Normal-Normal conjugacy)\n",
    "\n",
    "Let $y$ be a random variable that is drawn from a normal distribution with mean $\\mu$ and standard deviation $\\sigma$. \n",
    "\n",
    "1. Consider a dataset $Y=(y_1,y_2,...,y_n)$ containing $n$ realizations of $y$. Write down the likelihood of $Y$ as a function of $\\mu$ and  $\\sigma$\n",
    "\n",
    "2. Assume that $\\mu$ itself is Normally distributed around some mean $\\theta$ with standard deviation $\\tau$. Show that upon observing $Y$, the posterior belief about $\\mu$ remains normally distributed. Derive the posterior mean and standard deviation of $\\mu$. What do you notice about the standard deviation? Comment your finding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "1. The likelihood of $Y$ can be written as\n",
    "\n",
    "\\begin{align*}\n",
    "    L\\left(\\mu, \\sigma | Y \\right) &= \\prod_{i = 1}^{n} f\\left(y_{i}|\\mu, \\sigma\\right) \\\\\n",
    "                                   &= \\prod_{i = 1}^{n} \\frac{1}{\\sqrt{2\\pi \\sigma^{2}}}\\text{exp}\\left(-\\frac{\\left(y_{i}-\\mu\\right)^{2}}{2\\sigma^{2}}\\right).\n",
    "\\end{align*}\n",
    "\n",
    "2. By Bayes' rule, we find the posterior distribution of $\\mu$ given the data $Y$:\n",
    "\n",
    "\\begin{align*}\n",
    "     p\\left(\\mu|Y,\\theta,\\tau,\\sigma\\right) &\\propto p\\left(Y|\\mu,\\sigma\\right)p\\left(\\mu|\\theta,\\tau\\right)\\\\\n",
    "        & \\propto \\text{exp}\\left(-\\frac{1}{2}\\left[\\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2} + \\frac{\\left(\\mu-\\theta\\right)^{2}}{\\tau^{2}}\\right]\\right).\n",
    "\\end{align*}\n",
    "\n",
    "Let $\\overline{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_{i}$, by the Law of Total Variance, we have \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2} = \\sum_{i=1}^{n}\\left(y_{i} - \\overline{y}\\right)^{2} + n\\left(\\overline{y}-\\mu\\right)^{2}.\n",
    "\\end{equation*}\n",
    "\n",
    "Thus the posterior simplifies to:\n",
    "\n",
    "\\begin{equation*}\n",
    "    p\\left(\\mu|Y,\\theta,\\tau,\\sigma\\right) \\propto \\text{exp}\\left(-\\frac{1}{2}\\left[\\frac{n\\left(\\overline{y}-\\mu\\right)^{2}}{\\sigma^{2}} + \\frac{\\left(\\mu-\\theta\\right)^{2}}{\\tau^2}\\right]\\right).\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "The exponent can be written as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\left(\\mu - \\frac{\\frac{\\overline{y}}{\\sigma^{2}} + \\frac{\\theta}{\\tau^{2}}}{\\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}}\\right)^{2}*\\left(\\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}\\right).\n",
    "\\end{equation*}\n",
    "\n",
    "From this, we can conclude that the posterior mean $\\mu_{post}$ and the posterior variance $\\sigma^{2}_{post}$ read\n",
    "\n",
    "\\begin{align*}\n",
    "    & \\mu_{post} = \\frac{\\frac{\\overline{y}}{\\sigma^{2}} + \\frac{\\theta}{\\tau^{2}}}{\\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}}, \\\\\n",
    "    & \\sigma^{2}_{post} = \\frac{1}{\\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}}.\n",
    "\\end{align*}\n",
    "\n",
    "Note that the posterior variance is smaller than the prior variance, reflecting the fact that, after observing the data, we have more precise information about $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3 (Job Search)\n",
    "\n",
    "We consider a labor market where workers are either employed or searching\n",
    "for a job. Time is discrete and agents discount the future at rate $r$, so\n",
    "that their discount factor $\\beta =1/(1+r).$ When a firm meets a job seeker,\n",
    "it offers her a constant wage contract and the firm-employee relationship \n",
    "**lasts forever**.\n",
    "\n",
    "At the **end** of each period:\n",
    "\n",
    "(i). Employed workers receive their wages.\n",
    "\n",
    "(ii). Unemployed workers receive their benefits $z=-1$ along with a job\n",
    "offer whose wage is sampled from the following distribution\n",
    "\\begin{equation*}\n",
    "w=\n",
    "\\left\\{ \n",
    "\\begin{array}{l}\n",
    "1\\text{ with probability } p, \\\\ \n",
    "0\\text{ with probability } 1-p.%\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation*}\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Write down the Bellman equation for an unemployed worker and\n",
    "for a worker employed at wage $w$. Reinsert the value function $E(w)$ of\n",
    "employed workers into that of unemployed workers $U$ to express the latter\n",
    "as a function of $w$.\n",
    "\n",
    "2. Consider the following two job searching strategies. One\n",
    "where the worker accepts all job offers, which we denote by $U^{A}$; and one\n",
    "where the worker rejects low-wage offers ($w=0$), which we denote by $U^{R}.$\n",
    "Compute the values of $U^{A}$ and $U^{R}.$\n",
    "\n",
    "3. Under which condition on $p$ is it optimal for workers to\n",
    "reject low-wage offers? Provide an economic intuition for your result.\n",
    "(Hint: Remember that $z=-1.$)\n",
    "\n",
    "4. We now assume that the worker search for only two periods.\n",
    "What is the value function $U_{2}$ of an unemployed worker in the second\n",
    "and last search period?\n",
    " \n",
    "5. Consider now the value of being unemployed in the first\n",
    "search period $U_{1}$. Again, compare two job searching strategies: one\n",
    "where the worker accepts all job offers ($U_{1}^{A}$); and one where the\n",
    "worker rejects low-wage offers ($U_{1}^{R}$). Compute the two value\n",
    "functions.\n",
    "\n",
    "6. Under which condition is it optimal to reject low-wage offers\n",
    "in the first period? Compare your answer to the solution in 3. Provide\n",
    "an interpretation for your finding.\n",
    "\n",
    "7. We now assume that workers are uncertain about the probability \n",
    "$p$ at which they receive high-wage offers. Their initial belief at the\n",
    "beginning of period 1 is given by\n",
    "\\begin{equation*}\n",
    "p=\\left\\{ \n",
    "\\begin{array}{l}\n",
    "p_{h}\\text{ with probability } \\mu _{1}, \\\\ \n",
    "p_{l}\\text{ with probability } 1-\\mu _{1},\n",
    "\\end{array}\n",
    "\\right. \n",
    "\\end{equation*}\n",
    "where $p_{h}>p_{l}.$ Use Bayes' rule to express the agent's posterior $\\mu\n",
    "_{2}(0)$ at the beginning of period $2$ after she has received a low\n",
    "wage-offer. Show that $\\mu _{2}(0)<\\mu _{1}.$ In particular, what is the value of $\\mu_{2}(0)$ when $p_{h}=1?$\n",
    "\n",
    "8. We assume that $p_{h}=1$. Use Bayes' rule to compute $U_{1}^{R}$ and $U_{1}^{A}$. Under which condition is it optimal to reject a\n",
    "low-wage offer in period 1?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "1. The Bellman equation for an unemployed worker can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "    U = \\frac{1}{1+r}(z + p*\\text{max}\\{U, E(\\overline{w})\\} + (1-p)*\\text{max}\\{U, E(\\underline{w})\\}),\n",
    "\\end{align*}\n",
    "where $\\overline{w} = 1$ and $\\underline{w} = 0$. Reinserting the value function $E(w) = \\frac{w}{r}$ into $U$, we find that\n",
    "\n",
    "\\begin{equation*}\n",
    "    U = \\frac{1}{1+r}\\left(z + p*\\text{max}\\left\\{U, \\frac{\\overline{w}}{r}\\right\\} +(1-p)* \\text{max}\\left\\{U, \\frac{\\underline{w}}{r}\\right\\}\\right).\n",
    "\\end{equation*}\n",
    "\n",
    "2. The value of the job search srategy that accepts all job offer is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "    U^A = \\frac{1}{1+r}\\left(z + p\\frac{\\overline{w}}{r} + (1-p)\\frac{\\underline{w}}{r}\\right) = \\frac{p-r}{(1+r)r},\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where the equality follows reinserting $\\overline{w}=1$, $\\underline{w}=0$, and $z=-1$. Similarly, the value of the job search strategy that rejects low-wage offers is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "& U^R = \\frac{1}{1+r}\\left(z + p\\frac{\\overline{w}}{r} + (1-p)U^R\\right) \\Rightarrow U^R = \\frac{p-r}{(p +r)r}.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "3. Since the denominator of $U^R$ is smaller than the denominator of $U^A$, $U^R$ is greater than $UÂ$ whenever their numerators are positive, that is when $p>r$. Intuitively, the probability of receiving a high-wage offer must be high enough to compensate for the opportunity cost $r$ of remaining unemployed.\n",
    "\n",
    "4. In the second period, if the worker rejects the offer, she will stay unemployed forever and receive $\\frac{z}{r}<0=\\frac{\\underline{w}}{r}$. Therefore, workers will accept all offers in the second period so that $U_2 = U^A = \\frac{p-r}{(1 +r)r}$.\n",
    "\n",
    "5. $U_1^A$ and $U_1^R$ can be expressed as:\n",
    "\n",
    "\\begin{align*}\n",
    "    &  U_1^A = \\frac{p-r}{(1+r)r},\\\\\n",
    "    &  U_1^R = \\frac{1}{1+r}\\left(z + \\frac{p}{r} + (1-p)U_2\\right) = U_1^A + \\frac{1-p}{1+r}U_2.\n",
    "\\end{align*}\n",
    "\n",
    "6. For $U_1^R$ to be greater than $U_1^A$, the value of searching in the second and last period $U_2>0$ must be positive, which holds true whenever $p>r$. This is the same condition as in the infinite horizon problem derived in 3.\n",
    "\n",
    "7. According to Bayes' rule, $\\mu _{2}(0)$ can be expressed as:\n",
    "\n",
    "\\begin{align*}\n",
    "     \\mu _{2}(0) &= Pr(p=p_h| w_1=0)\\\\\n",
    "                 &= \\frac{Pr(w_1=0|p_h)Pr(p=p_h)}{Pr(w_1=0|p_h)Pr(p=p_h)+Pr(w_1=0|p_l)Pr(p=p_l)}\\\\\n",
    "                 &= \\frac{(1-p_h)\\mu_1}{(1-p_h)\\mu_1+(1-p_l)(1-\\mu_1)}.\n",
    "\\end{align*}\n",
    "\n",
    "The condition $\\mu _{2}(0)<\\mu _{1}$ is equivalent to $1/\\mu _1<1/\\mu _{2}(0)$. The second condition holds true whenever\n",
    "\n",
    "\\begin{align*}\n",
    "     \\frac{1}{\\mu_1} &< \\frac{(1-p_h)\\mu_1+(1-p_l)(1-\\mu_1)}{(1-p_h)\\mu_1}\\\\\n",
    "                 \\Leftrightarrow 1-p_h &< (1-p_h)\\mu_1+(1-p_l)(1-\\mu_1)\\\\\n",
    "                 \\Leftrightarrow 1-p_h &< 1-p_l,\n",
    "\\end{align*}\n",
    "which holds true by definition. As expected, workers put less weight on the optimistic prior $p=p_h$ after having received a low-wage offer. In particular, when $p_h=1$, $\\mu _{2}(0) = 0$ because workers can exclude the belief that they will always receive a high-wage offer.\n",
    "\n",
    "8. With $p_h=1$, $ U_1^A$ and $U_1^R$ can be expressed as:\n",
    "\n",
    "\\begin{align*}\n",
    "    &U_1^A = \\frac{1}{1+r}\\left(z + \\frac{\\mu_1}{r} + (1-\\mu_1)\\frac{p_l}{r}\\right),\\\\\n",
    "    &U_1^R = \\frac{1}{1+r}\\left(z + \\frac{\\mu_1}{r} + (1-\\mu_1)\\frac{p_l}{r}+(1-\\mu_1)(1-p_l)U_2(0)\\right),\n",
    "\\end{align*}\n",
    "where, according to the answer to question 4, $U_2(0) = U^A(0)=\\frac{1}{1+r}(z + \\frac{\\mu_2(0)}{r} + (1-\\mu_2(0))\\frac{p_l}{r})$. Since, $\\mu_2(0)=0$ when $p_h=1$, we have $U_2(0)=\\frac{p_l-r}{(1+r)r}$. Therefore, we find that\n",
    "\n",
    "\\begin{align*}\n",
    "    U_1^R > U_1^A \\Leftrightarrow U_2(0)=\\frac{p_l-r}{(1+r)r}>0.\n",
    "\\end{align*} \n",
    "\n",
    "The worker find it profitable to reject all low-wage offers in the first period when $p_l>r$. This condition is similar to the one in the environment without learning except that now the pessimistic belief has replaced the average one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgement**: This presentation is borrowed from Tim Miller's website [Mastering Reinforcement Learning](https://gibberblot.github.io/rl-notes/single-agent/multi-armed-bandits.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "A multi-armed bandit is defined by a set of random variables $X_{i,k}$ where: (i) $i\\in\\{1,2,..,N\\}$ is the arm of the bandit; (ii) $k$ is the index of the play.\n",
    "\n",
    "Draws are independently distributed and the gambler does not know the probability distributions of the random variables. The gambler plays succesive rounds, observes the reward from the selected arm after each round, and then adjusts her strategy with the objective of maximizing the rewards collected over all rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Value\n",
    "\n",
    "The **Q-value** of an action is defined as the **expected reward** for taking action $a$, i.e. $Q(a) = \\mathbb{E}[R | A = a]$. The true Q-value being unknown, it must be estimated. In practice, one can use the sample average of observed rewards to estimate the Q-value:\n",
    "\n",
    "\\begin{align*}\n",
    "Q(a) = \\frac{1}{N(a)} \\sum_{i=1}^{t} X_{\\{a,i\\}},\n",
    "\\end{align*}\n",
    "where $t$ is the number of rounds so far, $N(a)$ is the number of times action $a$ has been selected in previous rounds, and $X_{\\{a,i\\}}$ is the reward obtained in the $i-th$ round for playing arm $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration-Exploitation Dilemna\n",
    "\n",
    "After having explored her options for some time, the grambler forms a belief for the Q-value of each arm. Then she can switch to a greedy strategy, selecting only the actions that have given her the best rewards so far. However, being greedy may also lead the gambler to overlook the most profitable arm. Thus, she should follow a strategy that exploit the actions that have been profitable so far, while still exploring other actions. This tension is known as the **exploration vs. exploitation dilemma**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "We borrowed the code from Tim Miller's website [Mastering Reinforcement Learning](https://gibberblot.github.io/rl-notes/single-agent/multi-armed-bandits.html). A smaller repo containing all the programs required for this implementation is available [HERE](https://github.com/julprat/Armed_Bandits).\n",
    "\n",
    "The implementation of the strategy below inherits from the class `MultiArmedBandit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit():\n",
    "\n",
    "    \"\"\" Select an action for this state given from a list given a Q-function \"\"\"\n",
    "\n",
    "    def select(self, state, actions, qfunction):\n",
    "        abstract\n",
    "\n",
    "    \"\"\" Reset a multi-armed bandit to its initial configuration \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "\n",
    "To demonstrate the effect of a multi-armed bandit strategy, we use the simulation below. The simulation is an implementation of a simple multi-armed bandit problem with five actions and their associated probabilities. \n",
    "\n",
    "The simulation runs 2000 episodes of a bandit problem, with each episode being 1000 steps long. At each step, the agent must chose an action. At the beginning of each episode, the bandit strategies are reset. The simulation returns a list of lists, representing the reward received at each step of each episode. The aim for the bandit is to maximise the expected rewards over each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from qtable import QTable\n",
    "\n",
    "\n",
    "\"\"\" Run a bandit algorithm for a number of episodes, with each episode\n",
    "being a set length.\n",
    "\"\"\"\n",
    "\n",
    "def run_bandit(bandit, episodes=200, episode_length=500, drift=True):\n",
    "\n",
    "    # The actions available\n",
    "    actions = [0, 1, 2, 3, 4]\n",
    "\n",
    "    # A dummy state\n",
    "    state = 1\n",
    "\n",
    "    rewards = []\n",
    "    for _ in range(0, episodes):\n",
    "        bandit.reset()\n",
    "\n",
    "        # The probability of receiving a payoff of 1 for each action\n",
    "        probabilities = [0.1, 0.3, 0.7, 0.2, 0.1]\n",
    "\n",
    "        # The number of times each arm has been selected\n",
    "        times_selected = defaultdict(lambda: 0)\n",
    "        qtable = QTable()\n",
    "\n",
    "        episode_rewards = []\n",
    "        for step in range(0, episode_length):\n",
    "\n",
    "            # Halfway through the episode, change the probabilities\n",
    "            if drift and step == episode_length / 2:\n",
    "                probabilities = [0.5, 0.2, 0.0, 0.3, 0.3]\n",
    "\n",
    "            # Select an action using the bandit\n",
    "            action = bandit.select(state, actions, qtable)\n",
    "\n",
    "            # Get the reward for that action\n",
    "            reward = 0\n",
    "            if random.random() < probabilities[action]:\n",
    "                reward = 5\n",
    "\n",
    "            episode_rewards += [reward]\n",
    "\n",
    "            times_selected[action] = times_selected[action] + 1\n",
    "            qtable.update(\n",
    "                state,\n",
    "                action,\n",
    "                (reward / times_selected[action])\n",
    "                - (qtable.get_q_value(state, action) / times_selected[action]),\n",
    "            )\n",
    "\n",
    "        rewards += [episode_rewards]\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-greedy strategy\n",
    "\n",
    "The $\\epsilon$-greedy strategy balances exploration and exploitation by setting the parameter $\\epsilon$ which controls how much the gambler explores. Each time she selects an action, the gambler uses this strategy:\n",
    "\n",
    "1. With probability $1-\\epsilon$, the gambler **exploits** the armed-bandit by picking the arm with the maximum Q-value (in the case of a tie between several actions, the tie is broken randomly);\n",
    "2. With probability $\\epsilon$, the gambler **explores** by picking a random arm, using a uniform probability to select her pick.\n",
    "\n",
    "The $\\epsilon$-greedy strategy is implemented by the code below, using the `random()` function to pick a random number between $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "import random\n",
    "from multi_armed_bandit.multi_armed_bandit import MultiArmedBandit\n",
    "\n",
    "\n",
    "class EpsilonGreedy(MultiArmedBandit):\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def select(self, state, actions, qfunction):\n",
    "        # Select a random action with epsilon probability\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(actions)\n",
    "        arg_max_q = qfunction.get_argmax_q(state, actions)\n",
    "        return arg_max_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the reward for each step, averaged over 2000 episodes. Each episode is 1000 steps long. We evaluate different values of $\\epsilon$. The simulations illustrate the exploration-exploitation dilemna: High $\\epsilon$ have a lower reward in the long-run, but low $\\epsilon$ never converge to the opimal reward. \n",
    "\n",
    "For our parametric example, choosing $\\epsilon$ between 0.05 and 0.1 is a good strategy. However, there is no universal value for the $\\epsilon$ parameter, its optimal value varies across applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Untitled](_images/Armed_Bandits.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
